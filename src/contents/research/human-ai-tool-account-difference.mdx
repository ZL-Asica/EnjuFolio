---
title: 'Not All Places Are The Same: Computationally Surfacing Regional Experience Differences from Online Reviews'
authors:
  - Zhuoran Liu
  - Medini Chopra
  - Haoqi Zhang
advisors:
  - Dr. Haoqi Zhang (Northwestern University)
date: 2025-04
status: 'Under review'
role: 'Lead author'
abstract: >
  I led an HCI/NLP project that uses Yelp reviews and guidebook priors to model how the same place categories afford different experiences across U.S. regions, surfacing regional experiential signatures—like king cake, picon punch, or tax-free outlet shopping—that current category-only recommenders flatten.
url: 'https://dtr.northwestern.edu/projects/rec05fdcuNNGNmM8x'
keywords:
  - Experiential computing
  - Place-aware recommender systems
  - Online reviews
  - Geographic variation
  - Location-based services
  - Natural language processing
  - Human–computer interaction
---

## Overview

Working with my advisor Haoqi, I co-developed a project that treats location-based recommendation as a question about how **place imprints experience**, not just where venues are. Starting from travel guidebooks and HCI work on space versus place, I helped turn narrative difference claims—such as king cake being routine at bakeries in Louisiana but rare and “imported” in Pennsylvania, or tax-free shopping being a defining feature of Delaware outlets—into a concrete hypothesis set: for each experience phrase, compare how often and how concretely it appears in reviews for the _same_ Yelp category across states. That framing forced us to hold categories constant long enough to see meaning shift with geography, and to adopt a compact vocabulary of **environmental, cultural, social, and institutional drivers** to read whatever the text surfaced instead of treating it as anonymous signal.

To make this framing testable, I built an **end-to-end text pipeline** over the Yelp Open Dataset for thirteen U.S. states, cleaning millions of reviews, extracting short activity phrases as n-grams, and computing TF–IDF salience for each state–category document before using a stabilized log-ratio to quantify when a phrase is substantially more present in one region than another. Whenever the system returned brand chatter, implausible phrases, or weak contrasts, I iterated on thresholds, blacklist rules, and phrase definitions, pairing every numerical contrast with ranked review snippets so that both our team and future readers could audit whether “king cake,” “picon punch,” or “no sales tax” really reflected what people do in that place. The resulting case portfolio shows that everyday review language reliably encodes **regional experiential signatures**, but also exposes where affective claims like a beach feeling “personal” fail to appear at all—limits that shaped our design implications for future systems: interfaces that expose experiential facets alongside ratings, adjust defaults by region and season, and keep textual evidence visible rather than hiding behind opaque scores.

## Selected visuals

| Category           | Region    | Surfaced phrase (experience)             | Dominant driver          |
| ------------------ | --------- | ---------------------------------------- | ------------------------ |
| Bakeries           | Louisiana | **king cake** around Mardi Gras season   | Cultural / temporal      |
| Outlet malls       | Delaware  | **no sales tax** / cross-border shopping | Institutional / economic |
| Basque restaurants | Nevada    | **picon punch** as a pre-dinner ritual   | Social / cultural        |

Each row shows how, once we hold the Yelp category fixed and contrast review phrases across states, we recover distinct _regional experiential signatures_ that we then interpret through our four-driver vocabulary (environmental, cultural, social, institutional).
